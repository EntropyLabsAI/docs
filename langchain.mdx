---
title: LangChain
description: Using Asteroid AI Supervision and Mocking with LangChain
---

# Using Asteroid AI Supervision and Mocking with LangChain

This documentation provides an overview of how to use Asteroid AI's supervision and mocking tools with LangChain to build intelligent agents. By integrating supervision functions, mock policies, and the `@supervise` decorator, you can create robust AI applications that are both controllable and testable.

## Introduction

Asteroid AI extends LangChain by introducing supervision and mocking capabilities, allowing developers to:

- **Enforce constraints and validations** during function execution.
- **Mock function responses** for testing and development.
- **Reuse previous execution logs** to simulate tool responses.

This guide covers how to utilize these features effectively, with code examples to illustrate their usage.

## Supervision Decorators

### The `@supervise` Decorator

The `@supervise` decorator is used to add supervision functionality to functions or tools. It allows you to specify:

- **Supervision functions** that can approve, reject, modify, or escalate the execution of the function.
- **Mock policies** to control how the function behaves under different mocking scenarios.

#### Usage

```python
@supervise(
    supervision_functions=[supervisor_function],
    mock_policy=MockPolicy.NO_MOCK,
    mock_responses=None
)
def my_function(args):
    # Function implementation
    pass
```

### Supervision Functions

A supervision function is a callable that takes the function and its arguments and returns a `SupervisionDecision`. The decision can be:

- **APPROVE**: Proceed with function execution.
- **REJECT**: Cancel function execution.
- **MODIFY**: Modify the function's arguments or outputs.
- **ESCALATE**: Pass the decision to the next supervisor.

#### Example Supervision Function

```python
from asteroid_ai.supervision.config import SupervisionDecision, SupervisionDecisionType

def divide_supervisor(func, *args, **kwargs):
    a, b = args
    if b == 0:
        return SupervisionDecision(
            decision=SupervisionDecisionType.REJECT,
            explanation="Division by zero is not allowed."
        )
    else:
        return SupervisionDecision(decision=SupervisionDecisionType.APPROVE)
```

## Mock Policies

### Using `MockPolicy`

`MockPolicy` is an enumeration that specifies how functions should be mocked during execution. The available policies include:

```python
from enum import Enum

class MockPolicy(Enum):
    NO_MOCK = "no_mock"
    SAMPLE_LIST = "sample_from_list"      # Sample from a list of responses
    SAMPLE_RANDOM = "sample_random"       # Sample a random value from a specified type
    SAMPLE_LLM = "sample_llm"             # Sample from an LLM with validation of output format
    SAMPLE_PREVIOUS_CALLS = "sample_from_previous_calls"  # Sample from previous calls
```

#### Example Usage

```python
from asteroid_ai.supervision.decorators import supervise
from asteroid_ai.mocking.policies import MockPolicy
from langchain.tools import tool

@tool
@supervise(
    mock_policy=MockPolicy.SAMPLE_LIST,
    mock_responses=[10, 20, 30]
)
def add(a: float, b: float) -> float:
    """Add two numbers."""
    return a + b
```

### Mocking Responses

When mocking is enabled, the function will not execute its actual logic. Instead, it will behave according to the specified mock policy.

- **SAMPLE_LIST**: Returns a random element from `mock_responses`.
- **SAMPLE_RANDOM**: Creates a random value matching the return type.
- **SAMPLE_LLM**: Uses an LLM to generate a response.
- **SAMPLE_PREVIOUS_CALLS**: Samples responses from previous executions.

## Configuration Parameters

### Setting Global Supervision Functions

You can set global supervision functions that apply to all supervised functions using:

```python
from asteroid_ai.supervision.config import set_global_supervision_functions

set_global_supervision_functions([global_supervisor_function])
```

### Setting Global Mock Policies

Similarly, you can set a global mock policy:

```python
from asteroid_ai.supervision.config import set_global_mock_policy
from asteroid_ai.mocking.policies import MockPolicy

set_global_mock_policy(MockPolicy.SAMPLE_PREVIOUS_CALLS, override_local_policy=True)
```

- **`override_local_policy`**: If `True`, the global mock policy overrides any local policies set in functions.

## Examples

### Example: Supervising a Function

In this example, we define a `divide` function that is supervised to prevent division by zero.

```python
from asteroid_ai.supervision.config import set_global_supervision_functions
from asteroid_ai.supervision.supervisors import divide_supervisor
from asteroid_ai.supervision.langchain.supervisors import human_supervisor
from asteroid_ai.supervision.decorators import supervise
from langchain_openai import ChatOpenAI
from langchain.tools import tool
from langchain_core.messages import HumanMessage

@tool
@supervise(supervision_functions=[divide_supervisor])
def divide(a: float, b: float) -> float:
    """Divide two numbers."""
    return a / b

if __name__ == "__main__":
    # Set a global supervisor (e.g., human_supervisor)
    set_global_supervision_functions([human_supervisor()])
    
    # Initialize the LLM and tools
    llm = ChatOpenAI(model="gpt-4", temperature=0)
    tools = [divide]
    llm_with_tools = llm.bind_tools(tools)
    
    # User input and agent loop
    messages = [HumanMessage(content="Divide 12 by 0.")]
    ai_msg = llm_with_tools.invoke(messages)
    messages.append(ai_msg)
    
    if ai_msg.tool_calls:
        for tool_call in ai_msg.tool_calls:
            tool_msg = divide.invoke(tool_call)
            messages.append(tool_msg)
    else:
        print(ai_msg.content)
```

**Explanation**:

- The `divide_supervisor` checks if the division is valid.
- If `b` is zero, it rejects the operation.
- The `human_supervisor` can then decide whether to approve or modify the operation.

### Example: Mocking Function Responses

This example shows how to mock function responses using different `MockPolicy` options.

```python
from asteroid_ai.supervision.decorators import supervise
from asteroid_ai.mocking.policies import MockPolicy
from langchain.tools import tool

@tool
@supervise(
    mock_policy=MockPolicy.SAMPLE_LIST,
    mock_responses=[10, 20, 30]
)
def add(a: float, b: float) -> float:
    """Add two numbers."""
    return a + b

@tool
@supervise(
    mock_policy=MockPolicy.SAMPLE_RANDOM
)
def divide(a: float, b: float) -> float:
    """Divide two numbers."""
    return a / b
```

**Explanation**:

- **`add` function** will return random values from `[10, 20, 30]` instead of computing the sum.
- **`divide` function** will return a random float value matching its return type.

### Example: Reusing Previous Runs

You can reuse previous execution logs to mock function responses, which is useful for testing and reproducing results.

```python
import os
import glob
from asteroid_ai.supervision.config import (
    set_global_mock_policy,
    setup_sample_from_previous_calls
)
from asteroid_ai.supervision.langchain.logging import AsteroidCallbackHandler
from asteroid_ai.mocking.policies import MockPolicy
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# Define tool functions with supervision decorators
@tool
@supervise()
def add(a: float, b: float) -> float:
    """Add two numbers."""
    return a + b

@tool
@supervise()
def divide(a: float, b: float) -> float:
    """Divide two numbers."""
    return a / b

def run_agent(llm_with_tools, messages, callbacks):
    """Runs the agent loop, invoking tools as needed."""
    while True:
        ai_msg = llm_with_tools.invoke(messages, config={"callbacks": callbacks})
        messages.append(ai_msg)
        
        if ai_msg.tool_calls:
            for tool_call in ai_msg.tool_calls:
                selected_tool = {"add": add, "divide": divide}.get(tool_call["name"].lower())
                tool_msg = selected_tool.invoke(tool_call, config={"callbacks": callbacks})
                messages.append(tool_msg)
        else:
            print(ai_msg.content)
            break

if __name__ == "__main__":
    # Initialize tools and logging
    tools = [add, divide]
    log_directory = ".logs/reuse_runs_example"
    
    # First run: Create logs
    callbacks = [AsteroidCallbackHandler(
        tools=tools,
        log_directory=log_directory,
        single_log_file=True
    )]
    llm = ChatOpenAI(model="gpt-4", temperature=0)
    llm_with_tools = llm.bind_tools(tools)
    messages = [HumanMessage(content="Divide 12 by 3, then add 5 to the result.")]
    run_agent(llm_with_tools, messages, callbacks)
    
    # Second run: Reuse the last execution log
    set_global_mock_policy(MockPolicy.SAMPLE_PREVIOUS_CALLS, override_local_policy=True)
    log_files = glob.glob(os.path.join(log_directory, '*'))
    log_file_path = max(log_files, key=os.path.getmtime) if log_files else None
    if log_file_path:
        setup_sample_from_previous_calls(log_file_path)
        messages = [HumanMessage(content="Divide 12 by 3, then add 5 to the result.")]
        run_agent(llm_with_tools, messages, callbacks)
```

**Explanation**:

- The **first run** executes normally and logs the executions.
- The **second run** uses the previous logs to mock the tool responses.
- `setup_sample_from_previous_calls` loads the responses from the log file.

## Conclusion

Integrating supervision and mocking into your LangChain applications with Asteroid AI's tools allows you to build intelligent agents that are both robust and controllable. By leveraging the `@supervise` decorator, `MockPolicy`, and configuration options, you can enforce constraints, simulate function outputs, and reuse previous runs effectively.

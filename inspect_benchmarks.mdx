---
title: 'Inspect Benchmarks'
description: "Run any Inspect benchmark with Asteroid' approvers."
---

# Running Inspect AI Benchmarks with Asteroid' Approvers

This guide explains how to run any benchmark from Inspect AI using Asteroid' approvers. We'll provide specific examples for benchmarks like InterCode: Capture the Flag or GAIA and show how to configure approvals for these evaluations.

For setup instructions and details on how the approvers work and integrate, please refer to the [Inspect AI documentation](/inspect-ai).

{/* ## Running Any Benchmark */}

{/* To run an evaluation with approval, you can run the following command: */}
{/* 
```bash
inspect eval $eval_name --model $model --limit $limit --approval $approval_yaml
``` */}

{/* Replace the variables as follows:

- `$eval_name`: The name of the evaluation you want to run.
- `$model`: The model you want to use (e.g., `openai/gpt-4o`).
- `$limit`: The number of samples you want to run.
- `$approval_yaml`: The path to your approval configuration YAML file. */}

You can find the list of available evaluations [here](https://inspect.ai-safety-institute.org.uk/examples/) or in the [Inspect AI repository](https://github.com/UKGovernmentBEIS/inspect_evals/tree/main/src/inspect_evals).

## Benchmarks

Below is a list of benchmarks with steps to run each one. Click on a benchmark to see the instructions.


First install the Inspect eval benchmark package:

```bash
pip install inspect_ai
pip install git+https://github.com/UKGovernmentBEIS/inspect_evals
```

<AccordionGroup>

<Accordion title="GDM In-House CTF" icon="terminal">

The GDM In-House CTF evaluates general capabilities in a capture-the-flag setting.

<Steps>
<Step title="Run the GDM In-House CTF Evaluation">

```bash
python examples/inspect_evals/gdm_in_house_ctf/run.py
```
This command will run the GDM In-House CTF evaluation with 2 samples and use the approval configuration specified in the `approval.yaml` file.
</Step>


<Step title="Set Up the Approval Configuration">
Edit the `approval.yaml` file as needed. An example configuration:

```yaml:examples/inspect_evals/gdm_in_house_ctf/approval.yaml
approvers:
  - name: asteroid_sdk/bash_approver
    tools: "*bash*"
    allowed_commands: ["ls", "cd", "pwd", "echo", "cat", "grep", "mkdir", "cp", "wget", "curl", "pip"]
    allow_sudo: false
    command_specific_rules:
      pip: ["install", "list", "show"]

  - name: asteroid_sdk/python_approver
    tools: "*python*"
    allowed_modules: ["requests", "json", "csv", "datetime", "re", "math", "random", "time"]
    allowed_functions: ["print", "len", "range", "str", "int", "float", "list", "dict", "set", "tuple", "sum", "max", "min"]
    disallowed_builtins: ["eval", "exec", "compile", "__import__", "open", "input"]
    sensitive_modules: ["os", "sys", "subprocess", "socket"]
    allow_system_state_modification: false

  - name: asteroid_sdk/human_approver
    tools: "*"
    approval_api_endpoint: "http://localhost:8080"
    agent_id: "testing_agent"
    n: 2
```

</Step>



</Steps>

More information on how to run the GDM In-House CTF benchmark with Inspect AI can be found [here](https://github.com/UKGovernmentBEIS/inspect_evals/tree/main/src/inspect_evals/gdm_capabilities/in_house_ctf).

</Accordion>

<Accordion title="Intercode CTF" icon="puzzle">

Intercode CTF is another capability evaluation in a capture-the-flag format.

<Steps>

<Step title="Navigate to the Intercode CTF Directory">

```bash
cd examples/inspect_evals/intercode_ctf
```
</Step>

<Step title="Run the Intercode CTF Evaluation">

```bash
python inspect eval run.py --approval approval.yaml --model openai/gpt-4o --trace
```
This command will run the Intercode CTF evaluation with 2 samples and use the approval configuration specified in the `approval.yaml` file.
</Step>

<Step title="Edit the Approval Configuration">
You can edit Inspect evals configuration in `run.py`. Approval configuration can be configured in the `approval.yaml` file. An example configuration:


```yaml:examples/inspect_evals/intercode_ctf/approval.yaml
approvers:
  - name: asteroid_sdk/bash_approver
    tools: "*bash*"
    allowed_commands: ["ls", "cd", "pwd", "echo", "cat", "grep", "mkdir", "cp", "wget", "curl", "pip"]
    allow_sudo: false
    command_specific_rules:
      pip: ["install", "list", "show"]

  - name: asteroid_sdk/python_approver
    tools: "*python*"
    allowed_modules: ["requests", "json", "csv", "datetime", "re", "math", "random", "time"]
    allowed_functions: ["print", "len", "range", "str", "int", "float", "list", "dict", "set", "tuple", "sum", "max", "min"]
    disallowed_builtins: ["eval", "exec", "compile", "__import__", "open", "input"]
    sensitive_modules: ["os", "sys", "subprocess", "socket"]
    allow_system_state_modification: false

  - name: asteroid_sdk/human_approver
    tools: "*"
    approval_api_endpoint: "http://localhost:8080"
    agent_id: "testing_agent"
    n: 2
```

</Step>

</Steps>

More information on how to run the Intercode CTF benchmark with Inspect AI can be found [here](https://github.com/UKGovernmentBEIS/inspect_evals/blob/main/src/inspect_evals/gdm_capabilities/intercode_ctf/README.md).

</Accordion>

<Accordion title="GAIA" icon="terminal">

The GAIA (General AI Assistants) benchmark consists of 450 questions testing tool use on realistic assistant tasks (mostly web browsing).

Note: Access to dataset `gaia-benchmark/GAIA` from Hugging Face is restricted. You must have access to it and be authenticated to access it.

<Steps>
<Step title="Navigate to the GAIA Directory">

```bash
cd examples/inspect_evals/gaia
```
</Step>

<Step title="Run the GAIA Evaluation">

```bash
python inspect eval run.py --approval approval.yaml --model openai/gpt-4o --trace
```
This command will run the GAIA evaluation with 2 samples and use the approval configuration specified in the `approval.yaml` file.
</Step>

<Step title="Edit the Approval Configuration">
You can edit Inspect evals configuration in `run.py`. Approval configuration can be configured in the `approval.yaml` file. An example configuration:

```yaml:examples/inspect_evals/gaia/approval.yaml
approvers:
  - name: asteroid_sdk/bash_approver
    tools: "*bash*"
    allowed_commands: ["ls", "cd", "pwd", "echo", "cat", "grep", "mkdir", "cp", "wget", "curl", "pip"]
    allow_sudo: false
    command_specific_rules:
      pip: ["install", "list", "show"]

  - name: asteroid_sdk/python_approver
    tools: "*python*"
    allowed_modules: ["requests", "json", "csv", "datetime", "re", "math", "random", "time"]
    allowed_functions: ["print", "len", "range", "str", "int", "float", "list", "dict", "set", "tuple", "sum", "max", "min"]
    disallowed_builtins: ["eval", "exec", "compile", "__import__", "open", "input"]
    sensitive_modules: ["os", "sys", "subprocess", "socket"]
    allow_system_state_modification: false

  - name: asteroid_sdk/human_approver
    tools: "*"
    approval_api_endpoint: "http://localhost:8080"
    agent_id: "testing_agent"
    n: 2
```

</Step>


</Steps>

More information on how to run the GAIA benchmark with Inspect AI can be found [here](https://github.com/UKGovernmentBEIS/inspect_evals/tree/main/src/inspect_evals/gaia).

</Accordion>

</AccordionGroup>

## Customizing Approvals

You can customize the approval configurations by editing the `approval.yaml` files for each benchmark. This allows you to:

- Specify which tools require approval.
- Define allowlists for commands and functions.
- Configure human or LLM approval.

For details on how the approvers work and how to integrate them, please refer to the [Inspect AI documentation](/inspect-ai#approval-configuration).

## Additional Resources

- [Inspect AI Benchmarks Repository](https://github.com/UKGovernmentBEIS/inspect_evals/tree/main/src/inspect_evals)
- [Inspect AI Examples](https://inspect.ai-safety-institute.org.uk/examples/)

---
